{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aid Escalating Final Evaluation - 3 (Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Important Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import langdetect\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Training data\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Test data\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing ? with Nan in Train\n",
    "df.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing ? with Nan in Test\n",
    "df_test.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                                     0\n",
       "link_id                                  0\n",
       "page_description                         0\n",
       "alchemy_category                      1397\n",
       "alchemy_category_score                1397\n",
       "avg_link_size                            0\n",
       "common_word_link_ratio_1                 0\n",
       "common_word_link_ratio_2                 0\n",
       "common_word_link_ratio_3                 0\n",
       "common_word_link_ratio_4                 0\n",
       "compression_ratio                        0\n",
       "embed_ratio                              0\n",
       "frame_based                              0\n",
       "frame_tag_ratio                          0\n",
       "has_domain_link                          0\n",
       "html_ratio                               0\n",
       "image_ratio                              0\n",
       "is_news                               1688\n",
       "lengthy_link_domain                      0\n",
       "link_word_score                          0\n",
       "news_front_page                        727\n",
       "non_markup_alphanumeric_characters       0\n",
       "count_of_links                           0\n",
       "number_of_words_in_url                   0\n",
       "parametrized_link_ratio                  0\n",
       "spelling_mistakes_ratio                  0\n",
       "label                                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total Nan Values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "#Appending extra words to Stopwords\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords.append('title')\n",
    "lst_stopwords.append('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP preprocessing to carry out Punctuation Removal, LOwercase Conversion, Lemmatization, Tokenization\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = text.lower()\n",
    "#     text = re.sub(\"[\\W,\\d]\",\" \",str(text).lower().strip()) \n",
    "    text = re.sub(\"[\\W,\\d]\",\" \",text)\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "\n",
    "                \n",
    "#     ## Stemming (remove -ing, -ly, ...)\n",
    "#     if flg_stemm == True:\n",
    "#         ps = nltk.stem.porter.PorterStemmer()\n",
    "#         lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "    while \"  \" in lst_text:\n",
    "        lst_text = lst_text.replace(\"  \",\" \")\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying previous function on Train data\n",
    "df[\"text_clean\"] = df[\"page_description\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Applying previous function on Test data\n",
    "df_test[\"text_clean\"] = df_test[\"page_description\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text = df[\"text_clean\"]\n",
    "# test_text = df_test[\"text_clean\"]\n",
    "# complete_text = pd.concat([df[\"text_clean\"], df_test[\"text_clean\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=0)\n",
    "import nltk\n",
    "# train = df1\n",
    "\n",
    "#Retokenizing text to convert into Doc2vec format\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['text_clean']), tags=[r.label]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text_clean']), tags=[r.label]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove all punctuations from the text\n",
    "# import string as st\n",
    "# def remove_punct(text):\n",
    "#     return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "# df[\"page_description\"]  = df[\"page_description\"].apply(lambda x: remove_punct(x))\n",
    "# df_test[\"page_description\"]  = df_test[\"page_description\"].apply(lambda x: remove_punct(x))\n",
    "\n",
    "\n",
    "# #To remove all digits \n",
    "\n",
    "# import re\n",
    "# def rem_digits(text):\n",
    "#     text = re.sub(\"\\d\",\" \",text)\n",
    "#     return text\n",
    "\n",
    "# df[\"page_description\"]  = df[\"page_description\"].apply(lambda x: rem_digits(x))\n",
    "# df_test[\"page_description\"]  = df_test[\"page_description\"].apply(lambda x: rem_digits(x))\n",
    "    \n",
    "    \n",
    "# #Remove url/title related words   \n",
    "# def rem_url(text):\n",
    "#     text = re.sub(\"http|https|url|title|www\",\" \",text)\n",
    "#     return text\n",
    "# df[\"page_description\"]  = df[\"page_description\"] .apply(lambda x: rem_url(x))\n",
    "# df_test[\"page_description\"]  = df_test[\"page_description\"] .apply(lambda x: rem_url(x))\n",
    "\n",
    "\n",
    "# ''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
    "#     on special characters, tabs or any other string based on which text is to be seperated into tokens.\n",
    "# '''\n",
    "# def tokenize(text):\n",
    "#     text = re.split('\\s+' ,text)\n",
    "#     return [x.lower() for x in text]\n",
    "\n",
    "\n",
    "# import re\n",
    "# df[\"page_description\"]  = df[\"page_description\"].apply(lambda msg : tokenize(msg))\n",
    "# df_test[\"page_description\"]  = df_test[\"page_description\"].apply(lambda msg : tokenize(msg))\n",
    "\n",
    "\n",
    "# # Remove tokens of length less than 3\n",
    "# def remove_small_words(text):\n",
    "#     return [x for x in text if len(x) > 3 and len(x) < 11]\n",
    "\n",
    "\n",
    "# df[\"page_description\"] = df[\"page_description\"].apply(lambda x : remove_small_words(x))\n",
    "# df_test[\"page_description\"] = df_test[\"page_description\"].apply(lambda x : remove_small_words(x))\n",
    "\n",
    "# lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# lst_stopwords.extend([\"url\", \"youre\", \"dont\",\"havent\",\"hadnt\",\"wont\",\"wouldnt\",\"cant\",\"cannot\",\"can not\",\"im\",\n",
    "#                       \"m\",\"am\",\"ill\", \"i will\",\"its\",\"it is\",\"s\",\" is\",\"thats\", \"werent\", \"doesnt\", \"didnt\", \"hasnt\"\n",
    "#                      \"do not\",\"doesnt\",\"doesnot\",\"didnt\",\"didnot\",\"hasnt\",\"hasnot\",\"havent\",\"havenot\",\"hadnt\",\"wont\",\n",
    "#                       \"wouldnt\",\"im\",\"iam\", \"want\", \"onto\", \"into\", \"www\", \"url\", \"http\", \"https\"])\n",
    "# def remove_stopwords(text):\n",
    "#     return [word for word in text if word not in lst_stopwords]\n",
    "\n",
    "\n",
    "# df[\"page_description\"] = df[\"page_description\"].apply(lambda x : remove_stopwords(x))\n",
    "# df_test[\"page_description\"] = df_test[\"page_description\"].apply(lambda x : remove_stopwords(x))\n",
    "\n",
    "# # Apply lemmatization on tokens\n",
    "# def lemmatize(text):\n",
    "#     word_net = WordNetLemmatizer()\n",
    "#     return [word_net.lemmatize(word) for word in text\n",
    "            \n",
    "# df[\"page_description\"] = df[\"page_description\"].apply(lambda x : lemmatize(x))\n",
    "# df_test[\"page_description\"] = df_test[\"page_description\"].apply(lambda x : lemmatize(x))\n",
    "            \n",
    "# #con\n",
    "# m = [df[\"page_description\"][i] for i in range(0, len(df[\"page_description\"]))]\n",
    "# m1 = [df_test[\"page_description\"][i] for i in range(0, len(df_test[\"page_description\"]))]\n",
    "# x = [\" \".join(i) for i in m]\n",
    "# x1 = [\" \".join(j) for j in m1]\n",
    "# x\n",
    "            \n",
    "# df[\"page_description\"] = [i.split(\" \")for i in x]\n",
    "# df_test[\"page_description\"] = [i.split(\" \")for i in x1]\n",
    "            \n",
    "            \n",
    "# #Unique word Extraction\n",
    "# def unique(sequence):\n",
    "#     seen = set()\n",
    "#     return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "            \n",
    "            \n",
    "# df[\"page_description\"] = df[\"page_description\"].apply(lambda x : unique(x))\n",
    "# df_test[\"page_description\"] = df_test[\"page_description\"].apply(lambda x : unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary for Vectorization using Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3105/3105 [00:00<00:00, 3885236.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#Buiding vocab for Train data(After splitting actual train data)\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=6000, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [00:00<00:00, 2792010.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#Building vocab for Test data with labels (After splitting actual train data)\n",
    "# model_dbow2 = Doc2Vec(dm=0, vector_size=6000, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow.build_vocab([x for x in tqdm(test_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging each document with its unique key \n",
    "final_tagged = df_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text_clean']),tags = [r.alchemy_category]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2958/2958 [00:00<00:00, 3600334.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#Building vocab for Final tagged data\n",
    "model_dbow1 = Doc2Vec(dm=0, vector_size=6000, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow1.build_vocab([x for x in tqdm(final_tagged)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding & Vectorization using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents  = utils.shuffle(train_tagged)\n",
    "\n",
    "#Function for Vectorization using Doc2vec\n",
    "model_dbow.train(train_documents,total_examples=len(train_documents), epochs=60)\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents  = utils.shuffle(test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_documents  = utils.shuffle(test_tagged)\n",
    "# model_dbow2.train(test_documents,total_examples=len(test_documents), epochs=30)\n",
    "# def vector_for_learning2(model, input_docs):\n",
    "#     sents = input_docs\n",
    "#     targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "#     return targets, feature_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_documents_final= final_tagged\n",
    "# model_dbow1.train(test_documents_final,total_examples=len(test_documents_final), epochs=30)\n",
    "# def vector_for_learning1(model, input_docs):\n",
    "#     sents = input_docs\n",
    "#     targets ,feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "#     return targets , feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# #kf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "# search_space = {\"penalty\":['l1', 'l2', 'elasticnet', None],\n",
    "#                 \"solver\": ['liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#                 \"max_iter\" : [50,80,100,120],\n",
    "               \n",
    "#                 \"multi_class\" : ['auto', 'ovr', 'multinomial']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Train data\n",
    "y_train, X_train = vector_for_learning(model_dbow, train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Test data\n",
    "y_test, X_test = vector_for_learning(model_dbow, test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given test (Unknown label)\n",
    "y_test1,X_test1 = vector_for_learning(model_dbow,test_documents_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    " \n",
    "# importing machine learning models for prediction\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    " \n",
    "# importing voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding & Vectorization using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For Training data\n",
    "\n",
    "# word_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf = True, \n",
    "#     strip_accents = 'unicode',\n",
    "#     analyzer = 'word', \n",
    "#     token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n",
    "#     lowercase = False,  \n",
    "#     stop_words = 'english',  \n",
    " \n",
    "#     ngram_range = (1, 1), \n",
    "#     min_df = 5,\n",
    "#     max_df = 0.95,\n",
    "#     norm = 'l2',\n",
    "#     max_features = 8600\n",
    "# ) \n",
    "# x = word_vectorizer.fit_transform(m).toarray()\n",
    "# df_ = pd.DataFrame(x)\n",
    "# df_ \n",
    "\n",
    "# #For Testing data\n",
    "# word_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf = True, \n",
    "#     strip_accents = 'unicode',\n",
    "#     analyzer = 'word', \n",
    "#     token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n",
    "#     lowercase = False,  \n",
    "#     stop_words = 'english',  \n",
    " \n",
    "#     ngram_range = (1, 1), \n",
    "#     min_df = 5,\n",
    "#     max_df = 0.95,\n",
    "#     norm = 'l2',\n",
    "#     max_features = 8600\n",
    "# ) \n",
    "# x1 = word_vectorizer.fit_transform(m1).toarray()\n",
    "# df1_ = pd.DataFrame(x1)\n",
    "# df1_ \n",
    "\n",
    "# #Concatenating with Label\n",
    "\n",
    "# # df__ = pd.concat([df_, df[\"label\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding & Vectorization using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = gensim.models.Word2Vec(X_train,\n",
    "#                                    vector_size=5000,\n",
    "#                                    window=5,\n",
    "#                                    min_count=2)\n",
    "\n",
    "# words = set(w2v_model.wv.index_to_key )\n",
    "# X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "#                          for ls in X_train])\n",
    "# X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "#                          for ls in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modelling using Voting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing all the model objects with default parameters\n",
    "model_1 = LogisticRegression(max_iter=2000,\n",
    " multi_class= 'multinomial',\n",
    " penalty= 'l2',\n",
    " solver= 'sag')\n",
    "model_2 = XGBClassifier(booster='gbtree', callbacks=None,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
    "              early_stopping_rounds=None, enable_categorical=False,\n",
    "              eval_metric=None, feature_types=None, gamma=0.4,eta=0.2, gpu_id=-1,alpha=7,\n",
    "              grow_policy='lossguide', importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.05, max_bin=256,\n",
    "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "              max_depth=15, max_leaves=0, min_child_weight=3, missing=None,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=0,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "                    sampling_method='uniform',\n",
    "                     tree_method='hist', subsample=0.5, )\n",
    "model_3 = RandomForestClassifier(criterion=\"entropy\", max_depth=500, min_samples_split=10, n_estimators=5000)\n",
    "#model_4 = AdaBoostClassifier(n_estimators=150,learning_rate=0.01,algorithm=\"SAMME\")\n",
    "#model_5 = RidgeClassifier(alpha=5.0,fit_intercept=True, copy_X=True, max_iter=5000, tol=0.001, class_weight=\"balanced\", solver='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making the final model using voting classifier\n",
    "# final_model = VotingClassifier(\n",
    "#     estimators=[('lr', model_1), ('xgb', model_2), ('rf', model_3)], voting='soft', weights = [1.35,1.67, 0.65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_model = VotingClassifier(estimators=[('lr', model_1), ('xgb', model_2), ('rf', model_3),('ad', model_4) ,('rc', model_5)], voting='hard', weights = [1.9,1.7,1.67,1.33,1.45])\n",
    "final_model = VotingClassifier(estimators=[('lr', model_1), ('xgb', model_2), ('rf', model_3)], voting='hard', weights = [1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=2000,\n",
       "                                                 multi_class=&#x27;multinomial&#x27;,\n",
       "                                                 solver=&#x27;sag&#x27;)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(alpha=7, base_score=None,\n",
       "                                            booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=0.7,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False, eta=0.2,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=0....\n",
       "                                            interaction_constraints=&#x27;&#x27;,\n",
       "                                            learning_rate=0.05, max_bin=256,\n",
       "                                            max_cat_threshold=64,\n",
       "                                            max_cat_to_onehot=4,\n",
       "                                            max_delta_step=0, max_depth=15,\n",
       "                                            max_leaves=0, min_child_weight=3,\n",
       "                                            missing=None,\n",
       "                                            monotone_constraints=&#x27;()&#x27;,\n",
       "                                            n_estimators=300, n_jobs=0,\n",
       "                                            num_parallel_tree=1, ...)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(criterion=&#x27;entropy&#x27;,\n",
       "                                                     max_depth=500,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=5000))],\n",
       "                 weights=[1, 1, 2])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=2000,\n",
       "                                                 multi_class=&#x27;multinomial&#x27;,\n",
       "                                                 solver=&#x27;sag&#x27;)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(alpha=7, base_score=None,\n",
       "                                            booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=0.7,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False, eta=0.2,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=0....\n",
       "                                            interaction_constraints=&#x27;&#x27;,\n",
       "                                            learning_rate=0.05, max_bin=256,\n",
       "                                            max_cat_threshold=64,\n",
       "                                            max_cat_to_onehot=4,\n",
       "                                            max_delta_step=0, max_depth=15,\n",
       "                                            max_leaves=0, min_child_weight=3,\n",
       "                                            missing=None,\n",
       "                                            monotone_constraints=&#x27;()&#x27;,\n",
       "                                            n_estimators=300, n_jobs=0,\n",
       "                                            num_parallel_tree=1, ...)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(criterion=&#x27;entropy&#x27;,\n",
       "                                                     max_depth=500,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=5000))],\n",
       "                 weights=[1, 1, 2])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000, multi_class=&#x27;multinomial&#x27;, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=7, base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              early_stopping_rounds=None, enable_categorical=False, eta=0.2,\n",
       "              eval_metric=None, feature_types=None, gamma=0.4, gpu_id=-1,\n",
       "              grow_policy=&#x27;lossguide&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.05, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=15, max_leaves=0, min_child_weight=3, missing=None,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=300, n_jobs=0,\n",
       "              num_parallel_tree=1, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=500, min_samples_split=10,\n",
       "                       n_estimators=5000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(max_iter=2000,\n",
       "                                                 multi_class='multinomial',\n",
       "                                                 solver='sag')),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(alpha=7, base_score=None,\n",
       "                                            booster='gbtree', callbacks=None,\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=0.7,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False, eta=0.2,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=0....\n",
       "                                            interaction_constraints='',\n",
       "                                            learning_rate=0.05, max_bin=256,\n",
       "                                            max_cat_threshold=64,\n",
       "                                            max_cat_to_onehot=4,\n",
       "                                            max_delta_step=0, max_depth=15,\n",
       "                                            max_leaves=0, min_child_weight=3,\n",
       "                                            missing=None,\n",
       "                                            monotone_constraints='()',\n",
       "                                            n_estimators=300, n_jobs=0,\n",
       "                                            num_parallel_tree=1, ...)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(criterion='entropy',\n",
       "                                                     max_depth=500,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=5000))],\n",
       "                 weights=[1, 1, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Fitting \n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'voting':['hard', 'soft'],\n",
    "#           'weights':[(1,1,1,1), (2,1,1,1), \n",
    "#                      (1,2,1,1), (1,1,2,1),\n",
    "#                      (1,1,1,2), (1,2,1,1), \n",
    "#                      (1,1,2,2), (2,1,1,2)]}\n",
    "\n",
    "# #fit gridsearch & print best params\n",
    "# grid = GridSearchCV(final_model, params)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print('\\n')\n",
    "# print(f'The best params is : {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params came out to be 'hard' and (1,1,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_90 = final_model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_check = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8040540540540541\n",
      "Testing F1 score : 0.8027717095518264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred_check))\n",
    "print('Testing F1 score : {}'.format(f1_score(y_test, y_pred_check, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(y_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1695\n",
       "1    1263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = pd.concat([df_test['link_id'],x],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.to_csv(\"final_sub_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = final_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.996135265700483\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_train, y_pred_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelling using Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initializing the model\n",
    "# lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Best Params after Hyperparameter tuning \n",
    "# GS = GridSearchCV(estimator = lr,\n",
    "#                  param_grid = search_space,\n",
    "#                  scoring = [\"r2\",\"neg_root_mean_squared_error\",\"accuracy\"],\n",
    "#                  refit = \"r2\",\n",
    "#                  cv = 5, \n",
    "#                  verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model Fitting \n",
    "# GS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Best Params \n",
    "# GS.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Best Score \n",
    "# GS.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7905405405405406\n",
      "Testing F1 score : 0.7904502155254269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibab/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
    "# y_test, X_test = vector_for_learning(model_dbow, test_documents)\n",
    "\n",
    "# logreg = LogisticRegression(max_iter=50,\n",
    "#  multi_class= multinomial,\n",
    "#  penalty= 'l1',\n",
    "#  solver= 'saga')\n",
    "# logreg.fit(X_train, y_train)\n",
    "# y_pred = logreg.predict(X_test)\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "# print('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Prediction\n",
    "# y_pred_t = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.879549114331723\n"
     ]
    }
   ],
   "source": [
    "# print('Testing accuracy %s' % accuracy_score(y_train, y_pred_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vector Learning \n",
    "# y_test1,X_test1 = vector_for_learning(model_dbow,test_documents_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final prediction \n",
    "# y_pred_final = logreg.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.DataFrame(y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1569\n",
       "1    1389\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sub = pd.concat([df_test['link_id'],x],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>4257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>10236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>5494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>9302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>2633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      link_id  0\n",
       "0        4049  1\n",
       "1        3692  0\n",
       "2        9739  0\n",
       "3        1548  1\n",
       "4        5574  1\n",
       "...       ... ..\n",
       "2953     4257  0\n",
       "2954    10236  0\n",
       "2955     5494  0\n",
       "2956     9302  0\n",
       "2957     2633  0\n",
       "\n",
       "[2958 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sub.to_csv('final_sub_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling using XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7912912912912913\n",
      "Testing F1 score : 0.7913294116033841\n"
     ]
    }
   ],
   "source": [
    "# y_pred1 = model.predict(X_test)\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# print('Testing accuracy %s' % accuracy_score(y_test, y_pred1))\n",
    "# print('Testing F1 score : {}'.format(f1_score(y_test, y_pred1, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_final_1 = model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = pd.DataFrame(y_pred_final_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1509\n",
       "1    1449\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sub34 = pd.concat([df_test['link_id'],x1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sub34.to_csv('final_sub_99.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling using Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    "#  # create regressor object\n",
    "# regressor = RandomForestRegressor()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "# search_space = {'n_estimators':[100,200,500],\n",
    "#                'criterion':[\"gini\", \"entropy\", \"log_loss\"],\n",
    "#                'max_depth':[None,50,100],\n",
    "#                 'max_features':[\"sqrt\", \"log2\", None],\n",
    "                \n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# GS1 = GridSearchCV(estimator = regressor,\n",
    "#                  param_grid = search_space,\n",
    "#                  scoring = [\"r2\",\"accuracy\"],\n",
    "#                  refit = \"r2\",\n",
    "#                  cv = 5, \n",
    "#                  verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GS1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the regressor with x and y data\n",
    "# regressor.fit(x_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred1 = model.predict(X_test)\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# print('Testing accuracy %s' % accuracy_score(y_test, y_pred1))\n",
    "# print('Testing F1 score : {}'.format(f1_score(y_test, y_pred1, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_final_1 = model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = pd.DataFrame(y_pred_final_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling using AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load libraries\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# # Import Support Vector Classifier\n",
    "# from sklearn.svm import SVC\n",
    "# #Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "# svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "# # Create adaboost classifer object\n",
    "# abc =AdaBoostClassifier(n_estimators=100, base_estimator=svc,learning_rate=0.25)\n",
    "\n",
    "# model_addf = abc.fit(X_train, y_train)\n",
    "# #Predict the response for test dataset\n",
    "# y_pred = model_addf.predict(X_test)\n",
    "\n",
    "\n",
    "# # Model Accuracy, how often is the classifier correct?\n",
    "# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initializing all the model objects with default parameters\n",
    "# model_1 = LogisticRegression(max_iter=700,\n",
    "#  multi_class= 'multinomial',\n",
    "#  penalty= 'l1',\n",
    "#  solver= 'sag')\n",
    "# model_2 = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "#               colsample_bylevel=0.5, colsample_bynode=0.5, colsample_bytree=1,\n",
    "#               early_stopping_rounds=None, enable_categorical=False,\n",
    "#               eval_metric=None, feature_types=None, gamma=0.4,eta=0.2, gpu_id=-1,alpha=7,\n",
    "#               grow_policy='depthwise', importance_type=None,\n",
    "#               interaction_constraints='', learning_rate=0.05, max_bin=256,\n",
    "#               max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "#               max_depth=15, max_leaves=0, min_child_weight=3, missing=None,\n",
    "#               monotone_constraints='()', n_estimators=300, n_jobs=0,\n",
    "#               num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "#                     sampling_method='uniform',\n",
    "#                      tree_method='hist')\n",
    "# model_3 = RandomForestClassifier(criterion=\"entropy\", max_depth=100, min_samples_split=5, n_estimators=1000)\n",
    "# #model_4 = AdaBoostClassifier(n_estimators=150,learning_rate=0.01,algorithm=\"SAMME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = VotingClassifier(estimators=[('lr', model_1), ('xgb', model_2), ('rf', model_3)], voting='hard', weights = [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
